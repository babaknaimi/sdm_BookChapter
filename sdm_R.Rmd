---
title: "The sdm R package for species distribution modelling"
author: |
  Babak Naimi\textsuperscript{1*}, Elham Ebrahimi\textsuperscript{2}, Miguel B. Araújo\textsuperscript{3,4,5}
affiliation: |
  \textsuperscript{1}Quantitative Biodiversity Dynamics (QBD), Department of Biology, University of Utrecht, Padualaan 8, Utrecht, 3584 CH, The Netherlands \\
  \textsuperscript{2}Wildlife Ecology and Conservation, Department of Environmental Sciences, Wageningen University 6708 PB, Wageningen, The Netherlands \\
  \textsuperscript{3}Department of Biogeography and Global Change, National Museum of Natural Sciences, CSIC, Madrid, Spain \\
  \textsuperscript{4}Rui Nabeiro Biodiversity Chair, MED Institute, University of Évora, Évora, Portugal \\
  \textsuperscript{*}Corresponding author: \texttt{naimi.b@gmail.com}  


output: 
  pdf_document:
    latex_engine: xelatex
bibliography: citations.bib
header-includes:
  - \usepackage{fontspec}
  - \setmonofont{Courier New}
  - \usepackage{ulem}
  - \usepackage{graphicx}
  - \usepackage{float}
  - \usepackage{mdframed}
  - \setlength{\parskip}{1em}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage[table,xcdraw]{xcolor}
  - \usepackage{fancyhdr}
  - \renewcommand{\thefootnote}{\fnsymbol{footnote}}
---

\begin{center}
    \textsuperscript{1}Quantitative Biodiversity Dynamics (QBD), Department of Biology, University of Utrecht, Padualaan 8, Utrecht, 3584 CH, The Netherlands \\
    \textsuperscript{2}Wildlife Ecology and Conservation, Department of Environmental Sciences, Wageningen University 6708 PB, Wageningen, The Netherlands \\
    \textsuperscript{3}Department of Biogeography and Global Change, National Museum of Natural Sciences, CSIC, Madrid, Spain \\
    \textsuperscript{4}Rui Nabeiro Biodiversity Chair, MED Institute, University of Évora, Évora, Portugal \\
    \textsuperscript{5}Theoretical Sciences Visiting Program, Okinawa Institute of Science and Technology
Graduate University, Onna, 904-0495, Japan \\
    \vspace{0.3cm}
    \textsuperscript{*}Corresponding author: \texttt{naimi.b@gmail.com}
\end{center}


\maketitle


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Global biodiversity change is one of the most pressing environmental issues of our time [@pecl2017biodiversity; @Dornelas2023] as the planet is facing the largest biodiversity loss, known as the sixth mass extinction [@Kolbert2014], with the potential loss of half the species on the planet [@Hughes2023] and the third most important risk threatening humanity over the next 10 years (World Economic Forum, 2022). In response to such threats, global environmental authorities and the Convention on Biological Diversity (CBD) in the fifteenth meeting of the Conference of the Parties (COP15) launched a global commitment, the Kunming-Montreal Global Biodiversity Framework (KM-GBF), sets ambitious goals and targets to halt and reverse biodiversity loss by 2030 and 2050, aiming to restore ecosystems, conserve species, and ensure sustainable use of natural resources.

Quantitative analyses and sophisticated modelling tools play a significant role to inform biodiversity conservation strategies and policy decisions [e.g., @whittaker2005conservation]. Species Distribution Models (SDMs), the most common and widely used approach for biodiversity modelling [@araujo_standards_2019], have emerged as pivotal tools in biodiversity research and assessments, offering robust methods to predict the current geographic distributions of species across landscapes and project them into different times and spaces. These models, also known as bioclimatic envelope models, ecological niche models and habitat suitability models [for a review of terminologies see @araujo2012uses], explore the relationships between species observations (usually in the form of presence-only, presence-absence, or abundance) and environmental variables to provide insights into the potential distribution of species across geographic space and time [@ceballos_biological_2017; @dirzo_defaunation_2014; @guisan_predicting_2005; @guisan_predictive_2000]. The application of SDMs spans a wide range of tasks, such as forecasting or hindcasting the effects of climate change on biodiversity [@ebrahimi2022assessing; @thorup2021response], inferring risks from species invasion [@naimi_potential_2022], selecting sites for species conservation [@araujo2000selecting], habitat restoration [@zellmer2019predicting], rewilding [@araujo2024expanding], and species translocation [@chauvenet2013maximizing], as well as quantifying combined effects of anthropogenic factors on biodiversity [e.g., @hof2011additive; @taheri2021improvements], among many other applications [for a review see @Peterson2012].

The implementation of SDMs involves a workflow that integrates biological and environmental data, statistical and/or machine learning methods, and computational tools to predict species distributions. The `sdm` R package [@naimi_sdm_2016] provides a comprehensive and extensible  platform for developing SDMs using three commonly used types of species data: presence-absence, presence-only, and abundance. The package facilitates implementing the entire species distribution modelling workflow including data integration, modelling and evaluating their performance using multiple accuracy metrics, and using the models for certain applications (Fig. \ref{fig:Fig1}). This package supports a variety of modelling algorithms, including machine learning techniques such as Random Forests [RF, @breiman2001random], Boosted Regression Trees [BRT, @friedman2001greedy], Flexible Discriminant Analysis [FDA, @hastie1994flexible], Mixture Discriminant Analysis [MDA, @hastie1996discriminant], Radial Basic Function Neural Network [RBF, @hudak1992rce], Multi-layer Perceptron Neural Networks [MLP, @rosenblatt1958perceptron], Maxlike [@royle2012likelihood], MaxEnt [@phillips_maximum_2006], and MaxNet [@phillips2017opening], as well as classical statistical methods such as Generalized Linear Models [GLM, @mccullagh2019generalized] and Generalized Additive Models [GAM, @wood2017generalized], and also older family of SDM methods such as Bioclim [@busby1991bioclim], and Domain [@carpenter1993domain]. Furthermore, multiple ensemble forecasting methods [@araujo_ensemble_2007] are supported in the package to combine multiple predictions from different algorithms and replications to generate a consensus spatial distribution of species. By offering a flexible and user-friendly interface, the `sdm` R package supports multiple common formats of spatial and temporal data, facilitates developing SDMs by simultaneously incorporating multiple algorithms, enables automation of model fitting for multiple species simultaneously and comparison of their performance, generates different types of consensus across ensembles, and provides tools for interpretation of models, all enhancing the robustness and accuracy of predictions.


This book chapter describes the workflow of species distribution modelling and practical steps using the `sdm` R package. We provide a comprehensive guide, from data preparation and selection of environmental variables to model building, evaluation, and prediction. Emphasizing practical application, this chapter walks the reader through the utilization of various modeling algorithms supported by the sdm package, including techniques for optimizing model performance and interpreting results. By illustrating the usage of the package with a case study, we highlight the versatility and effectiveness of SDMs in biodiversity research and conservation planning. 

## An overview of the SDM workflow in the sdm R package:

The `sdm` R package is a comprehensive framework that provides a set of user-friendly functions for implementing the entire species distribution modelling workflow to develop data-driven SDMs, begins with providing data for both species (response or dependent variable) and environment (predictor or explanatory variables; also called covariates). The workflow consists of three major steps to build SDMs including data preparation, modelling, and prediction. The `sdm` package offers a user-friendly function for each step in the workflow with arguments controlling the tasks given a user’s inputs.

(a) \uline{\textit{Data preparation}}: involves controlling and integrating species and environmental data, identifying biases and cleaning issues with data [@rocchini2023quixotic] such as mismatch in coordinate reference systems, duplication of records, missing values, and testing collinearity among predictor variables [@naimi_where_2014]. Different types of species data are supported in the sdm package including **presence-only**, **presence-absence**, and **abundance** data. Since presence-only is the most widely used data type, a case study in this book chapter is demonstrated based on using this data type. In case of presence-only, pseudo-absence or background records can be generated at this stage. The main function for this stage in the sdm package is \texttt{sdmData}  which supports both tabular (e.g., \texttt{data.frame}) and spatial data formats, and can do all the above-mentioned tasks for cleaning and preparing data.

(b) \uline{\textit{Modelling}}: utilises various (20+) statistical and machine learning algorithms to construct SDMs based on the prepared clean data in the previous step (a). Once the models are built (also called training of the models), they undergo rigorous evaluation to assess their performance and predictive accuracy. This step often includes resampling procedure for dividing data into training and test partitions using multiple methods such as sub-sampling, cross-validation, and bootstrapping [@naimi_sdm_2016], which employ widely used evaluation metrics, like the Area Under the Curve (AUC) and True Skill Statistic (TSS), to ensure robust model validation [@allouche2006assessing; @fielding1997review]. The single function of `sdm` in the package integrates all the modelling and evaluation tasks defined in this phase that can be simultaneously applied for multiple species, multiple modelling methods, and multiple replications of data.

(c) \uline{\textit{Prediction}}: After a successful evaluation, the models are used for predicting (or projecting or hindcasting) species distributions across geographic areas and time periods under current and future environmental scenarios. The predict function in the `sdm` package can generate the predicted values (e.g., probability of occurrence) based on each trained model given the predictor variables as inputs. When multiple modelling methods are employed, the predictions of individual models can be combined into a single consensus prediction using ensemble prediction approach [@araujo_ensemble_2007]. The ensemble function in the sdm package offers several methods to combine individual models such as weighted and unweighted mean, median, mean of predicted presence-absence, two step mean, etc. (check the help page of the function in the sdm package for more information).

(d) \uline{\textit{Interpretation}}: Finally, the workflow may include post-processing steps for model interpretation and visualization. This can involve mapping predicted distributions, analyzing variable importance, and generating graphical representations of model outputs which facilitate understanding and communication of results to address specific applied questions. The `sdm` R package streamlines these processes, providing researchers and practitioners with powerful tools to develop, evaluate, and apply SDMs effectively to inform various applications, such as identifying priority areas for conservation, assessing the impacts of climate change, and guiding habitat restoration efforts. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Fig1.png}
    \caption{A schematic representation of the species distribution modeling workflow with the main functions, followed by their basic usages, in the \textit{'sdm'} R package.}
    \label{fig:Fig1}
\end{figure}


## Case study: Species Distribution Modelling of \textit{Snow leopard}

To demonstrate the SDM workflow, this section is dedicated to assess and explore \textbf{impacts of climate change on geographical distribution of Snow leopard} (scientific name: \textit{Panthera uncia}). We use the sdm R package, along with some other packages, to conduct the study, and here, a step by step guideline is provided to discuss the practical solution for implementing the workflow and generating results in the forms of maps, tables, and graphs.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{snow-leopard-picture.jpg}
    \caption{Snow leopard (Panthera uncia).}
    \label{fig:Fig1}
\end{figure}

The solution to conduct the study has been provided following the main steps in the workflow defined in Figure \ref{fig:Fig1}. The solution is provided through the following sections:

(i) \uline{Downloading data}: Here, we use R and the geodata package to download species data from GBIF, and climate data (for both the current and future times) from Worldclim.

(ii) \uline{Preparing data}: Data are checked for some issues, and get ready as spatial datasets for conducting SDMs.

(iii) \uline{Developing models using the sdm R package}: The main workflow in the sdm package [@naimi_sdm_2016] to develop and evaluate SDMs are demonstrated.

(iv) \uline{Predict/Project the map of habitat suitability in current and future times}: We predict the current distribution of the species (habitat suitability), and project its distribution into a future climatic scenario (for the year 2100).

(v) \uline{Assess range shift in response to climate change}: Given the habitat suitability maps in both the current and future times, we can assess the magnitude and distribution of changes, and quantitatively measure range shifts due to climate change.
 


### (i) Downloading Species and Climate Data

In this section, we will use R to download occurrence data for the Snow Leopard (\textit{Panthera uncia}) from the Global Biodiversity Information Facility (GBIF) and climate data from the WorldClim database. These datasets will serve as the foundation for developing species distribution models and assessing the potential impacts of climate change on the Snow Leopard’s habitat.


\uline{\textit{Species Occurrence Data}}: We will use the sp_occurrence function in the geodata package to download Snow Leopard occurrences from GBIF. Occurrences are the locations where the species has been observed (mostly cover presence-only records), providing spatial points essential for building SDMs. Some descriptions are provided as comments within the code:


```{r}
library(geodata)

# let's first check whether any records available on GBIF without downloading:

sp_occurrence("Panthera","uncia", download = F)


# we have 196 records available, let's download them as a data.frame:

sp <-  sp_occurrence("Panthera","uncia")

#-------
# the sp data.frame has over 100 columns, corresponding to different information
# Some of the columns can be used to filter and exclude some records.
# For example, the column "basisOfRecord" is a Darwin Core term that refers to 
# the specific nature of the record and can refer to one of 6 classes

# Let's check our records to see their frequency for different classes:

table(sp$basisOfRecord)
 
# You may find the definition from the GBIF website:
# https://docs.gbif.org/course-data-use/en/basis-of-record.html

#------
# You can also check the years of records in our dataset:

table(sp$year)

# let's also check occurrenceStatuse column:

table(sp$occurrenceStatus) # we are sure now all records are presence!
# if it was not all "PRESENT", we could keep only "PRESENT" to use data as 
# a presence-only dataset
#---------------------------
###### Filtering some records:

# Given the information avaialble in "basisOfRecords" and "year" columns, we
# want to filter data and keep appropriate records. We may use other columns
# like coordinateUncertaintyInMeters; or we may use some specific packages 
# designed to clean GBIF data (e.g., CoordinateCleaner). But here, we do our 
# filtering based on the "basisOfRecords" and "year" columns.

# Based on the information we checked above, we keep records with 
# basisOfRecord == "HUMAN_OBSERVATIO" & year > 1970, we use the which function:

w <- which(sp$basisOfRecord == "HUMAN_OBSERVATION" & sp$year > 1970)

# w keeps the record number (row number in sp data.frame) that we need to keep

sp <- sp[w,] # only keep records specified in w

#------
## Get rid of all columns except coordinate columns (lon & lat)
# from all columns, we only keep 2 columns (lon, lat)

sp <- sp[,c('lon','lat')]

# we add another column (its name can simply be "species") to which we assign 
# 1 to all records in the column as they are presence-only:

sp$species <- 1

# let's check the head of the table (data.frame) to make sure it is fine:

head(sp)


# now, we convert the sp data.frame to Spatial Points using the vect function in
# the terra package (generates SpatVector object):

sp <- vect(sp, c('lon','lat'))

sp # sp, now, is a SpatVector object contining spatial points of species records


# let's generate a map to see the locations of the species records:

# We had the world map but if you don't, you can find it on Internet!
wld <- vect('world.shp') 


plot(wld)

points(sp,col='red')




```

\uline{\textit{Downloading Climate Data}}:

We are going to use climate data with 19 bioclimatic variables (represent different aspects of climate condition including mean annual temperature and precipitation, their seasonality and extreme conditions) available in the WorldClim dataset. The bioclim variables are extracted from monthly weather data (monthly minimum and maximum temperature and mean precipitation). The variables are described here: https://www.worldclim.org/data/bioclim.html

Given that the purpose of the instructions provided here is to demonstrate how the SDM workflow can be implemented, we will use a coarse resolution (10 minutes; approximately 20 km) dataset to make the computation fast. However, in practice, the resolution should be selected based on the scale of the study to ensure accurate results. The WorldClim dataset is available on the _worldclim.org_ website.

We can download the dataset for both current and future time periods. The future climate conditions are estimated using General Circulation Models (GCMs), which simulate the Earth's climate based on various physical processes and interactions within the atmosphere, ocean, and land surface. These models consider different scenarios known as Shared Socio-economic Pathways (SSPs), which reflect various potential future conditions based on socio-economic trends and policies. SSPs provide a comprehensive framework to analyse the impacts of different socio-economic developments on climate change and to explore mitigation and adaptation strategies.

The climate data for the current time (baseline), represent mean climate condition generated from weather data recorded between 1980-2000. To download the climate data for future times, we need to select the appropriate GCMs (we may select several or even all GCMs for a real study) and SSP scenarios that align with the objectives of our study. For example, we might choose SSP2-4.5, which represents a moderate scenario where socio-economic trends lead to stabilization in global emissions by mid-century. Alternatively, we could select SSP5-8.5, which represents a high-emission scenario. Under SSP5-8.5, greenhouse gas emissions continue to rise throughout the century, leading to significant increases in temperature and more extreme climate conditions (also called business as usual or worst-case scenario). This scenario is useful for understanding the potential impacts of unmitigated climate change and helps in planning for worst-case outcomes.

For this exercise, let's download climate data for both the current time (baseline) and future time (year 2100; it is generated based on 20 years of data between 2081-2100) for one of GCMs and SSP5-8.5. We can use the *worldclim_global* function to download climate data for the current time, and cmip6_world to download for the future. Check the help page of the functions for more details about the usage of these functions:

```{r}
# the following function, download bioclim layers with the resolution of 10 min.

########################################################
##-----> Downloadin climate for the Baseline (current time):
########################################################

bio <- worldclim_global(var="bio",res=10, path = getwd())

bio


# let's check the names of layers in the bio object
names(bio)

# As you can see, the bio object contains 19 bioclim variables representing 
# climate conditions for the current time. The names are long, have information
# about the version of the WorldClim dataset (WC_2.1) and resolution.

# Let's make the names simpler (i.e., bio1, bio2, ..., bio19)

# We can simply assign the new names:

names(bio) <- paste0('bio',1:19) # Do you know why we used the paste0 function?

# now, let's check again the names of our data object:
names(bio)

# Let's visualise the first layer (Mean Annual Temperature):


plot(bio[[1]],main='Bio1: Mean Annual Temperature (Current time)')

# and 12th layer (Mean Annual Precipitation)

# for this one, let's define an appropriate color palette:

cl <- colorRampPalette(c('gray','yellow','green','blue','darkblue'))


plot(bio[[12]],col=cl(200),main='Bio12:Mean Annual Precipitation (Current time)')



########################################################
#----> Downloading bioclim variables for the Future time:
########################################################

# To download bioclim for a future time, we need the following information:

## GCM: Which Climate Model? (20+ GCMs are available!, here we chose: "MIROC6")

## SSP: Which Climate Scenario? (4 different scenarios are available!)
#-------> ssp: 126, 245, 370, 585 (from "optimistic"....to.... "pessimistic")

## Year: Which future time (e.g., "2081-2100")

# we can also specify variables (here, we need bioclim; "bioc"), 

## res: resolution (10 arc min ~ equivalent to almost 20 km)
#-----------
# we can use cmip6_world function in the geodata package to download climate 
# variables for the future times:

biof <- cmip6_world(model="MIROC6", ssp='585',
                    time = "2081-2100", var = 'bioc', res=10, path=getwd())

biof 


# let's check the names of the biof object which contains bioclim variables of
# future (year: 2100) for the worst-case scenario (ssp585):

names(biof) 

# For our modelling practice, the names of bioclim layers in the current and 
# future time should be exactly the same, so since the order of the layers in 
# bio and biof is the same, we can simply assign the names of biof to be the 
# same as bio:

names(biof) <- names(bio)


# let's visualise precipitation for the future time:

plot(biof[[12]],col=cl(200), 
     main="Bio12: Mean Annual Precipitation (Year:2100; SSP-585)")

```



### (ii) Data preparation

After downloading data, we need to take some steps to prepare data for modelling. 

While statistical methods can be employed to identify outliers of the species data, a visual inspection of the map with species locations suggests that the points in North America may be outliers. This suspicion can be further validated by comparing these points with the species' range map provided by the IUCN. Therefore, we can exclude the outlier records. However, if the study area to train the model is selected and the outliers are located outside of the study area, the outliers will not be considered in the modelling procedure (i.e., no need to exclude them).


\uline{\textit{Selecting study area}}: 

Although the purpose of our case study is to generate **global** potential distribution of species, we may select a realistic study area which can be an area potentially accessible to the species. Since the background records are generated within the study area, it is important to select a reasonable area to train SDMs. We may use a biogeographical map to overlap presence records and select the polygons intersected which species presence locations and conider them as the study area. Alternatively, we can approximately specify the area by choosing its upper left and lower right which give us the minimum and maximum x and y coordinates to make an extent object using the terra package.

```{r}
library(terra)

plot(wld)

bnd <- ext(c(60,120,10,60)) # spatial extent with c(X_min, X_max, Y_min, Y_max)

points(sp,col='red')

plot(bnd,add=T,border='blue')

```


\uline{\textit{Cropping Climate Data to the extent defined as the study area}}:

As mentioned earlier, when the species records are presence-only, we need to consider a realistic area (areas potentially accessible for the species) to calibrate (train) SDMs where the background records are drawn. Here, the specified extent (the rectangle) is used as the study area to train SDMs but we will use the models to predict/project across the world.

The following code crops the bioclim variables of the current time within the specified extent (bnd object). We don't crop the future dataset (biof) as the model training uses on the variables in the current time:

```{r}

# the extent specified earlier:
bnd


bioc <- crop(bio, bnd) # crop bio object within the bnd extent

# let's visualise a layer from "bioc" and add the species occurrences to the map:

plot(bioc[[1]],main='Bio1 (cropped)')

points(sp,col='red')

```

\uline{\textit{Multi-collinearity}}:

When we have two or more numerical predictor variables that are strongly correlated, the data are subjected to the issue of multi-collinearity (also called collinearity) [@Dormann2013]. Some modelling methods, such as GLMs, may be sensitive to the collinearity issue as their parameterisation can be affected. Even if a modelling method is not sensitive, excluding the collinear variables is a good practice given the parsimony rule to avoid redundant information in the model which makes it un-necessarily complex.

Several approaches are available to deal with collinearity issue. One approach is to transform the environmental variables into a new set of variables using Principle Component Analysis (PCA), an ordination method for reducing data dimensions and generate new, uncorrelated variables. This can be done using the `pca` function, or by specifying the `pca` term in the formula in the `sdmData` function.

Another approach is to identify and exclude variables that show signs of collinearity. To identify collinear variables, a simple pairwise correlation test can be conducted between each pair of variables (a correlation greater than 0.7 is indicative of collinearity). Alternatively, the variance inflation factor (VIF) can be measured for each variable, representing how much of a variable's variance can be explained by other variables (a VIF greater than 10 is a sign of collinearity). 

A popular hybrid approach was introduced by Naimi et al. (2014) to measure VIF or combination of VIF and correlation coefficient through a stepwise procedure [@naimi_where_2014]. Two functions implemented in the usdm package can be used here including `vifstep`, and `vifcor`. To deal with collinearity, one of these two methods should be used. `vifstep` checks the VIF metric for all numerical predictor variables and identifies the variable with the maximum VIF for exclusion if it exceeds the specified threshold (default=10). This procedure is repeated step by step until all the remained variables have VIF values below the threshold.

Alternatively, `vifcor` checks the correlation coefficient for all possible pairs of variables, and identifies the pair with the maximum correlation. If it exceeds the specified threshold (e.g., 0.7), one of the two variables in the pair is excluded. To make decision that which one should be excluded, VIF is calculated for both variables in the pair and the one with the greater VIF is excluded. The procedure is repeated step by step until none of the remaining pairs have a correlation greater than the threshold [@naimi_where_2014].

Here, we test the collinearity issue among the 19 bioclim variables in the current time using the `vifstep` function, and exclude collinear variables from all datasets (current and future times):

```{r}
library(usdm) # contains the functions to deal with collinearity

v <- vifstep(bioc, th=10) # checks collinearity with vifstep and threshold of 10

# here is the output of vifstep, reporting which variables should be excluded:
v


# Now, let's exclude the collinear varibales from all data objects:
# ... exclude collinear variables from bioc (cropped bio) based on "v"
bioc <- exclude(bioc,v)

# now, you can see that bioc has the selected variables:

bioc


# let's do it for objects the objects of global scale (bio, biof):
bio <- exclude(bio, v)

biof <- exclude(biof, v)

# If you want to save the new data objects, you can use the writeRaster function
# Example:....> bio <- writeRaster(bio, filename="bioclim_selected_current.tif")

#-----------
# as you can see, the selected variables are in bio (and in biof and bioc):

bio
```

### (iii) Developing models using the sdm R package: 

Now, we have both species (response) and climate (predictors) variables ready to implement the modelling workflow using the sdm package. The package can be installed normally from CRAN or from Github using: `devtools::install_github('babaknaimi/sdm'))`. **To get all modelling methods available for training SDMs, some additional packages also need to be installed. You may use the `installAll()` function one time after installing sdm to get all required packages installed on your machine.**

In the sdm package, two user-friendly functions should be used to develop the models. First, an sdmdata object should be created using the `sdmData` function, then, models are trained and evaluated using the `sdm` function.

The usage of the function:

`sdmData(formula,train,predictors,test,bg,filename,crs,impute,metadata,...)`


The `sdmData` function requires both species and predictor variables and several issues with data (e.g., duplications, missing values) and inconsistencies (e.g., mis-match in coordinate reference systems) are checked and fixed by this function. It has a `formula` interface (check the following box) which provides flexibility for users to control data structure and formats, and apply several functions on data.


\begin{mdframed}[backgroundcolor=gray!10, linecolor=black!75, linewidth=2pt, roundcorner=5pt, shadow=true,frametitle={\textbf{BOX 1: \uline{Formula interface in the sdmData function}}}]

The "formula" interface in R is a powerful and versatile tool that simplifies the specification of statistical models. Central to many modeling functions in R, the formula interface allows users to define the relationship between the dependent and independent variables in a clear and concise manner. This interface uses a symbolic syntax to represent models, making it intuitive and accessible for users to specify complex models without delving into the underlying mathematical details.

At its core, the formula interface uses a simple syntax: the tilde (~) operator separates the response variable on the left from the predictor variables on the right. For example, in the formula y ~ x1 + x2, y is the dependent variable (species), and x1 and x2 are the independent variables (predictors; environmental variables). In the sdm R package, the formula interface has been extended to support specifying different types of data (e.g., categorical/factor variables, time, coordinates), and provide flexibility in controlling data processing and modelling by users. 

The formula interface is implemented as the first argument in the two main functions in the sdm package, including \texttt{sdmData}, and \texttt{sdm} Following syntax are examples to illustrate how the formula interface in the sdm package provides flexibility which contributes to make the package user-friendly.
The formula in both the  \texttt{sdmData}, and \texttt{sdm} functions is used to specify the species in the left-hand side, and predictors in the right-hand side. Let’s assume the name of species in our data is “species”, and we used three predictors named “bio1”, “bio2”, and “bio3”, then, the formula can be defined as:

\texttt{sdmData(species \textasciitilde{} bio1 + bio2 + bio3, …)}

We can simply use “.” in the right-hand side to avoid typing all the variables names, meaning use all existing variables:


\texttt{species \textasciitilde{} .}


\uline{\textit{Categorical variables}}: If we have categorical variables in our dataset, we can explicitly specify them by using the term of “factor” or simply “f” in the formula (refers to factor data type that is used in R for handling categorical data). Let’s assume our categorical variable is “landuse”, then the formula would be:


\texttt{species \textasciitilde{} . + f(landuse)}


\uline{\textit{Spatial coordinates}}: When we use spatial data objects (e.g., spatial points and raster), coordinates of species locations are extracted from the dataset, but when the input dataset is a data.frame and if spatial coordinates are available (e.g., two columns of the data.frame are coordinates with the names of “lon” and “lat”), we can specify them in the formula:

\texttt{species \textasciitilde{} . + f(landuse) + coords(lon+lat)}

\uline{\textit{Time (temporal data)}}: If data/time records are available (e.g., date of species observations), it can be specified using the term of “time”:

\texttt{species \textasciitilde{} . + f(landuse) + coords(lon+lat) + time(eventDate)}

\uline{\textit{Multiple species}}: The sdm package supports modelling for simultaneously multiple species. When we have multiple species in the dataset, their names can be included in the lef-hand side of the formula as:

\texttt{species1 + species2 + species3 \textasciitilde{} . + f(landuse)}

If the left-hand side of the formula is left empty, the package tries to detect the available species data in the input dataset by assuming all variables having binomial records (1 and 0), are species variables (presence-absence):

\texttt{\textasciitilde{} . + f(landuse)}


\uline{\textit{Selecting variables}}: Another formula term in the sdm package, “select”, can be used in data preparation procedure to select a subset of variables using a variable selection procedure. For instance, assessing the issue of multi-collinearity in data may be resulted to exclude some problematic variables and keep a subset. The multi-collinearity assessment in the sdm package uses two different approaches, “vifcor” and “vifstep” (Naimi et al., 2014), implemented in the usdm R package. By using “select”, a user can specify names of (numerical) variables for which the collinearity should be checked:
The following example checks collinearity for all numeric variables using the vifstep function given the threshold of 10:


\texttt{species \textasciitilde{} . + select(., method="vifstep",th=10) }

The following example checks collinearity for all numeric variables except “bio1” using the vifcor function given the threshold of 0.7:

\texttt{species\textasciitilde{} . + select(. – bio1, method="vifcor", th=0.7) }

\uline{\textit{Scaling the variables}}: Given that the range (gradient) of predictor variables is usually different and not directly comparable to each other, transforming them to a similar range (scaling) can significantly enhance model convergence and performance. Scaling ensures that each predictor variable contributes equally to the model, preventing variables with larger ranges from disproportionately influencing the results. This is especially important for modeling algorithms sensitive to the scale of input data, can also facilitate the optimization process, leading to faster and more stable convergence of the model parameters. 

In The sdm package, it is straightforward and easy to use the scale term in the formula to standardize predictor variables before model fitting. By using the scale function, users can ensure that their models are robust, efficient, and interpretable, as this step is a best practice in the species distribution modeling workflow, reinforcing the reliability and effectiveness of the models developed using the sdm package. Two scaling methods are available in the package, “minmax” (default) transforms the data to a range between 0 and 1, and “center” scales each variable by subtracting its mean, divided by its standard deviation:

\texttt{species \textasciitilde{} . + scale(.) \# scaling of all variables using the default method ("minmax") }

\texttt{species \textasciitilde{} . + scale(bio1 + bio2) \# scaling of only bio1 and bio2 variables}

\texttt{species \textasciitilde{} . + scale(., method="center") \# scaling all using the method of "center"}
 
\uline{\textit{Principle component analysis (PCA)}}: PCA is a valuable technique for reducing the dimensionality of data and addressing issues of multi-collinearity among predictor variables. In the context of species distribution modeling, PCA transforms the original numerical variables into a set of uncorrelated components, which can then be used as predictors in the model. This transformation helps in simplifying the model by retaining only the most significant components, thereby enhancing computational efficiency and model interpretability. The sdm R package facilitates the incorporation of PCA directly within the formula interface. By specifying a PCA term in the formula, users can automatically transform the numerical variables into principal components, selecting either the first n components or those that explain a specified portion of the total variance (e.g., 80%). The option to scale the variables before PCA further ensures that each variable contributes equally to the analysis. This integration of PCA within the sdm package streamlines the workflow, allowing for more effective data preprocessing and robust model development:

The following line of code re-scales and transforms all numeric variables using PCA and selects the first two generated components to be used as predictor variables in the SDM workflow:
 
\texttt{species \textasciitilde{} pca(. , n=2 , scale = TRUE) }

Alternatively if n = "auto", the function identifies the best number of components which is the default behaviour; i.e., the following line of code is equivalent to: species \textasciitilde{} pca(.) :

\texttt{species \textasciitilde{} pca(. , n= "auto", scale = TRUE)  }

Or you may specify the minimum percentage of variability explained by the selected components:

\texttt{species \textasciitilde{} pca(. , n=0.8 , scale = TRUE) }

By using the formula interface, users can leverage R's extensive statistical modeling capabilities with ease. This interface not only promotes consistency across different modeling functions but also enhances readability and reproducibility of the code. As such, the formula interface is an essential feature of R that empowers users to build, analyse, and interpret a wide array of statistical models efficiently.

\end{mdframed}


The sdmdata object, generated by the `sdmData` function, is like a database containing all species and environmental records as well as additional information (spatial coordinates, temporal dimension, grouping factors, coordinate reference system, metadata, etc.).

Let's create the `sdmdata` object given the species data (`SpatVector` object: `sp`) and the climate variables (`SpatRaster` object: `bioc`). In the species data (`sp`), we had the presence-only values (1) assigned to a column named `species`. 

\uline{\textit{Backgrounds (pseudo-absences)}}: Since the species dataset is presence-only, we need to generate background (pseudo-absence) records. Several methods are available to generate backgrounds (you may check the help page of the `background` function in the sdm package). For this purpose, we use the method of `gRandom` which generates background randomly in the geographic space. The setting to generate background records can be provided through the `bg` argument to the  `sdmData` function using a list. In addition to the method, `n` (the number of background records), and some optional arguments can be provided (e.g., `bias` which is a raster file can be used for a target-based background generation). 

```{r}
library(sdm)

## the first step in the sdm package is to create the data object:
# ---> 500 background records will be generated using "gRandom":
d <- sdmData(species~.,train=sp,predictors=bioc,bg=list(n=500,method='gRandom'))

# you may get a summary from the created object by executing the object:

d

```

\uline{\textit{Modelling and Evaluation}}: After creating the data object (`sdmdata`), the `sdm` function can be used to train and evaluate multiple models in parallel. 20+ modelling methods are available in the package. The models can be extended by users through using the `add` function. The list of existing methods can be checked using getmethodNames function:


```{r}
getmethodNames(alt=F)
```

Usage of `sdm`:

`sdm(formula, data, methods,...)`

In the sdm function, `formula` (to specify the structure of the model), `data` (the output of `sdmData`), and `methods` (to specify the names of modelling methods) are the minimum inputs to get the function works. Some additional inputs can be provided to specify settings for the `replication` methods which split data to training and test partitions for evaluating the models. The following sub-section provides some details about model evaluation:

\uline{\textit{Replication}}: To evaluate the models, if an independent dataset is available, thay can be used to test the performance of models. If that is the case, the independent test data can be provided to the `sdmData` function. However, such independent data are not available in most situations. Therefore, an alternative solution might be used which is to split the data into two partitions using a re-sampling method. Several re-sampling methods are available in the sdm package including "sub-sampling" (uses a random sampling without replacement to divide data given the percentage specified by a user; for example, if `test.percent` = 30 is specified by a user, 30% of records are randomly selected to be used as test data and the rest for training the models), "bootstrapping" (uses a random sampling with replacement; for this method, a random sample with replacement and with the same size as the original data is drawn from the records which are used for training the models, and the records that are not selected in the sample are used as the test data), and "cross-validation" (divide data into n folds/partitions [`cv.fold` specified by a user] at the beginning of the procedure and the models are trained n times and every time one of the folds is selected as the test data). A user can specify one or more of these methods through the `replication` argument, and the procedure can be repeated multiple times if a user add the `n` argument with the number of replications.


Here is an example of using the sdm function where we chose 5 modelling method, also used sub-sampling with 30% of data for test dataset, and repeated the procedure 3 times:



```{r}
## second step: to train and evaluate the models:

#----> 5 modelling methods are used: 
# =======>> glmp: polynomial generalised linear model; 
# =======>> brt: Boosted Regression Trees; 
# =======>> rf: Random Foress; 
# =======>> maxent: Maximum Entropy; 
# =======>> svm: Support Vector Machine; 

#----> Replication method: sub-sampling (with 30 % of test data; repeats 3 times)

m <- sdm(species~., d, methods = c('glm','brt','rf','maxent','svm'),
         replication='sub', test.p=30, n = 3)


# to see a summary of the modelling outputs, you may check the object:

m


```

As you can see above, in the summary information provided from the `sdmModels` object, 3 models were trained for each modelling method (15 models in total), and they were evaluated using multiple accuracy metrics (e.g., AUC, TSS). Mean values of these metrics can provide a quick way to compare the performance of different methods.

There is a user-friendly and interactive way to explore part of the results available within the sdmModels object through using the `gui` function. This function opens a graphical user interface with different parts (windows/tabs) which allows a user to explore the results (evaluation, variable importance, etc.). You may simply use the following line of code to use the function:

`gui(m)`

\
\
\


\begin{mdframed}[backgroundcolor=gray!10, linecolor=black!75, linewidth=2pt, roundcorner=5pt, shadow=true,frametitle={\textbf{BOX 2: \uline{Model evaluations for SDMs}}}]

Evaluating the performance of models is a crucial step in species distribution modeling (SDM) to ensure the accuracy and reliability of predictions. Several evaluation metrics are commonly used to assess how well a model predicts the occurrence of species based on known data. Here are some of the most widely used methods:


\textbf{\large Confusion Matrix and Threshold-Based Metrics}

The confusion matrix is a tool used to evaluate the performance of a models when the response variable is categorical (or classes such as presence and absence). The confusion matric tool allows to compare the observed values (presence/absences) with the predicted presence and absences. Since SDMs are usually generating probability of occurrences (or habitat suitability values) rather than presence/absence values, a threshold is needed to first convert the predicted probabilities to presence/absences, then make the confusion matrix and calculate the accuracy metrics. Therefore, the metrics generated based on a confusion matrix is also called \textbf{threshold-based} metrics. The matrix is composed of four key components:


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{cmx.png}
    \caption{Confusion Matrix: compares observed presence and absence records with predicted presence and absence records to measure multiple metrics representing accuracy of a model (e.g., sensitivity, specificity) }
    \label{fig:Fig3}
\end{figure}



\textbf{Description of the Metrics:}


- \uline{\textit{True Positives (TP)}}: The number of observed presences that are correctly predicted as presence by the model (i.e., the model correctly identifies the presence of the species).

- \uline{\textit{True Negatives (TN)}}: The number of observed absences that are correctly predicted as absence by the model (i.e., the model correctly identifies where the species is absent).
   
- \uline{\textit{False Negatives (FN)}}: The number of observed presences that are incorrectly predicted as absence (i.e., missed presences).

- \uline{\textit{False Positives (FP)}}: The number of absences where the model incorrectly predicts presence.


\textbf{Key Metrics Derived from the Confusion Matrix}:

- \uline{\textit{Accuracy}}: 
  \[
  \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
  \]
\textbf{Accuracy} measures the overall correctness of the model by calculating the proportion of correctly classified instances (both presences and absences) out of all predictions.

- \uline{\textit{Sensitivity (True Positive Rate or Recall)}}:
  \[
  \text{Sensitivity} = \frac{TP}{TP + FN}
  \]
\textbf{Sensitivity} evaluates how well the model identifies actual presences (true positives). A higher sensitivity indicates fewer missed presences.

- \uline{\textit{Specificity (True Negative Rate)}}:
  \[
  \text{Specificity} = \frac{TN}{TN + FP}
  \]
\textbf{Specificity} assesses how well the model identifies actual absences (true negatives). A higher specificity means fewer false positives (wrongly predicted presences).

- \uline{\textit{Precision or ppv (Positive Predictive Value)}}:
  \[
  \text{Precision} = \frac{TP}{TP + FP}
  \]
\textbf{Precision} measures the proportion of predicted presences that are true presences. A higher precision means that when the model predicts presence, it is more likely to be correct.

- \uline{\textit{F1 Score}}:
  \[
  \text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
  \]
The \textbf{F1} score is the harmonic mean of precision and recall, giving a balanced measure of the model's accuracy when there is a trade-off between false positives and false negatives.


- \uline{\textit{Matthews Correlation Coefficient (MCC)}}:
  \[
  \text{MCC} = \frac{(TP \times TN) - (FP \times FN)}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}
  \]

The \textbf{MCC} is a more balanced metric that considers all elements of the confusion matrix (TP, TN, FP, FN). It is particularly useful for imbalanced datasets, as it captures the relationship between true positives, true negatives, false positives, and false negatives. MCC ranges from -1 to 1, where 1 indicates perfect prediction, 0 means no better than random, and -1 means total disagreement between predictions and observations.

- \uline{\textit{True Skill Statistic or TSS}}:
  \[
  \text{TSS} = \text{Sensitivity} + \text{Specificity} - 1
  \]
    \textbf{TSS} is a widely used metric which evaluates model performance by combining sensitivity and specificity into a single measure. TSS ranges from -1 to 1, where 1 indicates perfect prediction, 0 indicates no better than random, and negative values indicate performance worse than random.


\textbf{Threshold Optimization}:

As mentioned, predictions from SDMs (those that are developed using presence-absence data) are in the form of probability (that may be interpreted as habitat suitability). Since probability values are not comparable with observed Presence/Absences, a threshold is needed to convert the probability values to presence-absences. Then, the question is that what is the best threshold! A threshold optimization method may be used to find the best threshold given some criteria available for this purpose. In the sdm R package, 15 different criteria are available (it can be specified in different functions through using the `opt` argument; check the help page of the `evaluates` function for more information). Here is a list of some widely used methods: 

For instance, we may find the threshold under which [sensitivity + specificity] is maximised (is known as: max[Se+Sp]).

Threshold optimization is crucial when converting continuous model predictions (e.g., probabilities of species presence) into binary classifications (presence/absence). Several approaches can be used for threshold optimization:

1) \uline{\textit{Se = Sp}}: This is one of the commonly used approach for threshold optimization, and it finds  the threshold where the sensitivity and specificity is equal. In the sdm package, this threshold can be used by setting `opt = 1`.

2) \uline{\textit{max[Se + Sp]}}: The is probably the most widely used approach, and it is the threshold that maximizes [sensitivity + specificity] which is equivalent to maximizing TSS. In the sdm package, this threshold can be used by setting `opt = 2` (default).

2) \uline{\textit{P10, P5, P1, and P0}}: These approached select the threshold based on percentile of probabilities across presence records in the evaluation dataset. P10, P5, P5, and P0 refer to 10, 5, 1, and 0 percentiles, respectively.


\textbf{\large Threshold-independent metrics}

The metrics mentioned so far are all threshold-based metrics, relying on finding the best threshold to first convert probability values to presence/absences, then creating a confusion matrix based on which multiple metrics can be calculated. Therefore, depending on which optimization approach is used, the metrics can vary. There is another group of metrics available to measure the performance of SDMs, with the advantage that there is no need to select a threshold. These threshold-independent metrics are often preferred when the goal is to evaluate the overall predictive ability of the model, regardless of specific classification cutoffs to convert probabilities to presence/absences.

- \uline{\textit{Area Under the Receiver Operating Characteristic Curve (AUC)}}: 

AUC (Area under the ROC curve) is probably the most widely used metric that measures the ability of SDMs to discriminate between presence and absence locations. AUC ranges between 0 and 1, with a value of 0.5 indicating a performance equivalent to random guessing, while values closer to 1 indicate better discriminative ability. AUC is calculated based on the ROC which plots the true positive rate (sensitivity) against the false positive rate (1-specificity) across all possible threshold values. AUC summarizes the curve into a single value, providing an overall assessment of model performance across thresholds.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{roc.png}
    \caption{Two examples of ROC curve with corresponding AUC (area under the ROC curve) values; AUC is a metric assessing discriminative capacity of a model; the lower panel shows the density plots corresponding to the ROC curves in the upper panel, representing the distribution of predicted values across presence and absence locations in an evaluation (test) dataset; when AUC is greater, the model has greater power in discriminating presence locations from absence locations}
    \label{fig:Fig4}
\end{figure}

- \uline{\textit{Correlation (COR)}}: 

Correlation is another threshold-independent metric that assesses the agreement between predicted and observed values. The most common measure used is Pearson's correlation coefficient, which ranges from -1 to 1, with 1 showing a perfect positive correlation, 0 showing no correlation, and -1 representing perfect negative correlation.

- \uline{\textit{Devience}}: 

This metric represents the average deviation of model predictions from observations, therefore, a smaller value of it representing a better model (can be interpreted as smaller error).

- \uline{\textit{Continuous Boyce Index (CBI)}}: 

The Continuous Boyce Index (CBI) is an evaluation metric specifically designed for **presence-only** test dataset. It assesses how much better the predicted model is compared to a random model. CBI ranges from -1 to 1, where values close to 1 indicate a model with high predictive ability, 0 indicates no better than random, and negative values suggest the model is predicting worse than random.

- \uline{\textit{Calibration}}:

Calibration measures reliability of predicted probabilities related to observed species prevalence or proportion of presences across the range of probabilities predicted by a model.  If predictions are perfectly calibrated, we expect that the estimated probabilities represent the actual prevalence of data. For instance, across the sites with predicted probabiliy of 0.7, we expect 70% of observations are presence and 30% absence. The calibration metric is a novel method implemented in the sdm package to quantify the level of calibration, ranges between 0 and 1. A value of 1 refers to a perfect calibration and a value of 0.5 refers to the perfect discrimination (all observations greater than a threshold is presence, and below a threshold is absence). A negative value may also be generated for rare or not-likely situations such as generating opposite predictions (i.e., high probabilities for absences and low probabilities for presences). The following Figure shows a realistic calibration plot (on the left) with some extreme examples to show the range of calibration metrics (small plots on the right):

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{calibration.png}
    \caption{The calibration plot with a novel metric developed in the sdm package representing the degree of calibration. The practical range is between 0.5 and 1.}
    \label{fig:Fig5}
\end{figure}



\end{mdframed}

### (iv) Predict/Project the map of habitat suitability in current and future times

After the models were trained and evaluated successfully, we can now use them to predict the probability of occurrence (it may be interpreted as the degree of habitat suitability) for each pixel across the study area. Here, the aim is to assess the geographic distribution of the species globally, therefore, we use the non-cropped predictor variables (`bio`) to predict the values in the current time:


```{r}
## Predict habitat suitability of Snow leopard in the current time:

p <- predict(m, bioc)

# since we had 15 models in the `m` object, `p` is a SpatRaster with 15 layers 
# corresponding to the 15 models:

p

# We have 5 modelling methods, and 3 models for each method (15 in total),
# the first 3 layers are for the first method ("glmp"), the secord 3 (4, 5, and 6)
# are related to the second method ("brt"), and so on.

# let's visualise one of the outputs for each method:

# appropriate color schemes:
cl <- colorRampPalette(c('gray','orange','yellow','green','blue')) 

## rasterVis for visualisation:
# So far, we used the simple plot function to visualise rasters,
# but with rasterVis, you may generate nicer plots:

library(rasterVis)

levelplot(p[[c(1,4,7,10,13)]],col.regions=cl(200),margin=F)


```

\uline{\textit{Ensemble forecasting}}: Employing multiple modelling methods for species distribution modelling which is usually applied to several replications of data (e.g., in the example above, we produced 15 models), often results in varying predictions. While these models may all perform well according to evaluation metrics (e.g., AUC), their predictions can still differ significantly. These variations in outcomes of models are referred to model-based uncertainty, which is one of the key sources of uncertainty affecting SDMs. 

To address this uncertainty, an effective approach is  ensemble forecasting, where the predictions from multiple models are combined into a single, consensus prediction [@araujo_ensemble_2007]. Ensemble forecasting helps to smooth out the variability in individual models, providing a more robust and reliable prediction of species distribution. According to Araújo and New (2007), ensemble methods can improve the accuracy and reliability of SDMs by leveraging the strengths of different models while minimizing their individual weaknesses.

\uline{\textit{Ensemble forecasting methods}}: In the sdm package, the `ensemble` function can be used for generating the consensus prediction that supports multiple methods for combining the predictions. Several approaches are available to combine predictions from multiple models in the `ensemble` function (the method is specified within a list passed through the `setting` argument:

- **Mean** (simple averaging): This is the simplest method, where the predictions from all models are averaged to produce the final consensus prediction. Each model contributes equally to the final result. To use this method, `method = 'mean'` or `method = 'unweighted'` can be used in the `setting` argument of the `ensemble` function. 


- **Weighted Mean**: In this method, models with better performance (as determined by evaluation metrics such as AUC or TSS) are given more weight in the final prediction. For example, models that perform better based on AUC would contribute more significantly to the final ensemble prediction than models with lower AUC scores. Examples of settings in the `ensemble` function include: `setting = list(method = 'weighted', stat = 'auc')` which uses AUC values as weight; OR `setting = list(method = 'weighted', stat = 'tss', opt=2)` which uses TSS values as weight. Since TSS is a threshold-based metric, the `opt` argument is also added which specifies which threshold optimisation method should be used to select the best threshold for calculating TSS. 15 optimisation methods are available in the sdm package (check the help page of the `evaluates` function), and `opt = 2` refers to the second method that selects threshold under which [`sensitivity` + `specificity`] is maximized (i.e., `max[se+sp]`).  

- **Median**: This method takes the median of predictions from all models. To use this method (`method = 'median'`). 

- **Thresholding and stacking**: This method involves thresholding predictions to determine areas of presence or absence given the best threshold for each model specified using a threshold optimisation method (the `opt` argument). The predicted presence-absence values from multiple models are stacked and averaged (`method = 'pa'`). Example: `setting = list(method = 'pa', opt=2)` 

- **Two-step averaging**: When multiple predictions are generated for each modelling method (from multiple data replications), the averaging of predictions can be done in two steps, across replications for each modelling method and then across modelling methods. The available two-step methods include `mean-weighted`, `mean-unweighted`, `median-weighted` and `median-unweighted`. For instance, using the `method = 'mean-weighted'` in the setting argument first takes the mean of different replications of each modelling methods, then they are combined through using a weighted-mean function.

- **Assess variability in predictions**: In addition to generating a consensus prediction, several methods are avaialble to explore the variability of predictions for each location. These methods include coefficient of variation (`method = 'cv'`), standard deviation (`method = 'stdev'`), confidence interval (`method = 'ci'`; this generates the range of confidence interval which is "upper range - lower range"), and uncertainty (`method = 'uncertainty'`). The uncertainty method can characterise model-based uncertainty or inconsistency among predictions of different models. An entropy metric is used to characterise uncertainty, ranging between 0 and 1. The value of 1 at a certain location refers to maximum inconsistency which is the case when half of the models predict 'presence' for the location and the other half predict 'absence'.

The above methods mentioned methods can be employed together at the same time.

Following is examples of how to generate an ensemble prediction using the `ensemble()` function in the sdm package. Please **note** that the `ensemble` function \uline{first calls the `predict` function to generate predictions}, and then use an ensemble method to combine them into a consensus map. However, if the predictions are generated beforehand, the output of the predict function can be introduced as the second argument to the ensemble function (in our case, `p`), otherwise, the predictors object (in our case, `bioc`) should be used as the second argument:


```{r}
## Ensemble:
#----> first argument: the model (output of the sdm function)
#---->  second argument: either the output of the predict function (if available)
#---------------------- OR the predictors object (bioc; )
#----> third argument (setting): a list with settings for ensemble procedure

### We fitst try 3 methods and compare them in a plot:

## (i) Weighted-mean using values of AUC as weights
en1 <- ensemble(m, p, setting = list(method='weighted', stat= 'auc'))

#----
## (ii) Weighted-mean using values of TSS as weights 
# opt=2 refers to the threshold obtained using max[Sensitivity + Specificity]

en2 <- ensemble(m, p, setting = list(method='weighted', stat= 'tss', opt=2))

## (iii) PA: thresholding and stacking:
en3 <- ensemble(m, p, setting = list(method='pa', opt=2))

#------ 
## Visualising the outputs of 3 different ensembles:


en <- c(en1,en2,en3) # 3 objects stacked in a single object
names(en) <- c('ens_AUC','ens_TSS','ens_PA')

levelplot(en,col.regions=cl(200),margin=F,
          main=c('Weighted-Mean (AUC)','Weighted-Mean (TSS)','Thresholding (PA)'))


#-------------------------------
## Assessing variability among predictions:

en4 <- ensemble(m, p, setting = list(method=c('mean','ci'), opt=2))

levelplot(en4,col.regions=cl(200),margin=F,
          main=c('Mean','Range of Confidence-interval'))

## to assess the upper and lower limits of confidence intervals:
ci.lower <- en4[[1]] - (en4[[2]] / 2)
ci.upper <- en4[[1]] + (en4[[2]] / 2)
ci <- c(ci.lower,ci.upper)
names(ci) <- c('ci_lower','ci_upper')

levelplot(ci,margin=F,
          main=c('CI.lower (alpha = 0.05)','CI.upper (alpha = 0.05)'))

#----------
## Uncertainty
en5 <- ensemble(m, p, setting = list(method='uncertainty', opt=2))

levelplot(en5, margin=F, main='Model-based Uncertainty')

```


As you can see above, a single map is generated by combining the 15 predicted rasters (output of the predict function) using the ensemble method specified in the setting. There are other optional arguments avaialable to control the ensemble procedure including:

- `id`: By default, predictions from all models are contributing to generate the consensus map unless a user specify the `id` of certain predictions (based on their `modelID`s). For example, adding `id = 1:10` to the `setting` list, only uses the first 10 predictions in the above example.

- `expr`: An expression can be specified by a user to control or limit which predictions should contribute into generating the consensus map. For example, using `expr = auc > 0.7` uses the specified condition to only include well-performing models (those with AUC > 0.7) contributing to the ensemble procedure.

- `power`: This argument is used to change weights by calculating `weights = weights ^ power`. A value of greater than 1, gives more emphasise on better performing models.

- Check the help page of the `ensemble` function in the `sdm` package for the other arguments and more details about the function.


### (v) Assessing range shifts of the species in response to climate change: 

In this section, the models are used to generate the spatial distribution of species globally across time (i.e., for both the current and future periods). SDMs can project potential changes in species' geographic ranges by incorporating climate change scenarios (e.g., different future climate scenarios or SSPs). These projections allow researchers to estimate how a species' suitable habitat might shift under changing climate conditions.

\uline{\textit{Key Considerations for Assessing Range Shifts:}}:


a) **Baseline Distribution vs. Future Projections**: The first step in assessing range shifts involves comparing the baseline distribution of a species (i.e., its current range) with its projected distribution under future climate scenarios. These projections are typically made for various future time periods, such as 2050 or 2100 (here in this chapter, we only do it for the year 2100), based on different climate models (GCMs) and emission scenarios SSPs (Shared Socioeconomic Pathways). The difference between the two distributions highlights potential range expansions, contractions, or shifts.

b) **Types of Range Shifts**:

- **Latitudinal Shifts**: Species may shift pole-ward in response to rising temperatures. For example, species in the northern hemisphere may move northward, while those in the southern hemisphere might shift southward.

- **Altitudinal Shifts**: As temperatures increase, species living in mountainous regions may move to higher elevations where cooler conditions prevail.

- **Range Contractions**: Some species might experience range contractions due to unsuitable environmental conditions, leading to local **extinctions**.

- **Range Expansions**: Conversely, some species may benefit from climate change, expanding (colonizing) into new areas that were previously too cold or otherwise unsuitable.

c) **Metrics for Quantifying Range Shifts**:


- **Magnitude of Change**: The change in the degree of suitability between the future and current times can be simply calculated as a map by subtracting the current probability from the future one. Each pixel shows the magnitude of gains (positive values) and losses (negative values) in the future. Visualising the map highlights areas where habitat suitability increases (gain), decreases (loss), or remains unchanged. This can help to visualize the areas most affected by climate change.
     \[
     \text{Magnitude of Change} = \text{Habitat Suitability in Future} - \text{Habitat Suitability in Current}
     \]

- **Assessing Expansion/Contraction**: By converting the habitat suitability values in both times to Presence/Absences, given a threshold, and comparing the values at each pixel, a pixel in future will be expanded (colonized) when species will be presence in the future while it is absence in the current time. Oppositely, the contraction (extinction) of a pixel in the future will be the case when species is presence in the current time wile it will become absence in the future.

- **Centroid Shift**: The geographic centroid of the species' range is calculated for both the current and future projections. The distance and direction of the shift in the centroid can provide insight into whether the species is moving poleward, altitudinally, or in another direction. To calculate distance, we can use an Euclidean distance function (the coordinates should be metric):
     \[
     \text{Centroid shift} = \sqrt{(X_f - X_c)^2 + (Y_f - Y_c)^2}
     \]
     where \(X_f, Y_f\) represent the coordinates of the future centroid, and \(X_c, Y_c\) represent the current centroid coordinates.
   
- **Range Overlap**: The degree of overlap between the current and future predicted range can be used to assess how much of the species’ original habitat remains suitable under climate change. Low overlap indicates substantial changes in habitat suitability.
     \[
     \text{Range overlap (\%)} = \frac{\text{Area of overlap}}{\text{Current range area}} \times 100
     \]
     
- **Range Area Change**: The change in the total area of suitable habitat is another key metric. This can be calculated by comparing the size of the species' predicted range in the future with its current range.
     \[
     \text{Range area change (\%)} = \frac{\text{Future range area} - \text{Current range area}}{\text{Current range area}} \times 100
     \]
     
\uline{\textit{Assessing Range Shifts for our case study}}:

For our case study, we have already fitted SDMs across a defined study area, but now we want to use these models to predict the potential distribution of the species **globally** for both the current and future times. We have already downloaded bioclimatic data for both the current period and future projections (year 2100 under the SSP585 scenario, which represents a high greenhouse gas emission scenario). This will allow us to assess how the species' range might shift under a potential future climate.

**Area of applicability**: Given that the SDMs were initially fitted for a study area, which was a subset of the entire globe, predicting across the global scale can introduce new challenges. When applying these models globally, it is likely that some areas will exhibit environmental conditions that were not represented within the original study area. This phenomenon is referred to as environmental extrapolation. It can affect the reliability of predictions and should be carefully considered when interpreting the results. To handle this, we can identify and restrict our predictions to areas where the environmental conditions are within the area of applicability (AOA) of the model, meaning that the conditions are similar to those found within the training dataset. To calculate AOA, the `aoa` function in the sdm package can be used which generates a raster map with pixel values ranging between 0 and 1. A value of 1 means that the pixel is within the range used to fit the model. Deviating from 1 toward 0 refers to a more dissimilarity of the environmental condition of a pixel compared to the condition used to fit SDMs.

Let's follow the range shift assessment globally step by step:


```{r}
## Predict spatial distribution for the current time (globally):

# the model object: m, 
# the global bioclim dataset for the current time: bio
# the global bioclim dataset for the current time: biof

#---> we directly use the ensemble (no need to first run predict function):
#------> so, we use bio (predictors) as the second argument

hc <- ensemble(m, bio, setting=list(method="weighted",stat="auc"))
names(hc) <- 'Current'

## Project spatial distribution to the future
hf <- ensemble(m, biof, setting=list(method="weighted",stat="auc"))
names(hf) <- 'Future'

cl <- colorRampPalette(c('gray','orange','yellow','green','blue')) 


levelplot(c(hc,hf),col.regions=cl(1000),margin=F)


## Area of applicability:

# to identify the unreliable pixels (check the Area of Applicability section):
# The aoa function needs the following arguments:
# ----> x: the raster predictors
#-----> d: the output of sdmData (or sdmModels)
#-----> vi: optional variable importance of the predictor variables

#-- let's first extract variable importance from the variables:

# in the id argument, we may specify modelIDs or "ensemble":
vi <- getVarImp(m,id='ensemble')

plot(vi)

vi@varImportance$corTest

vimp <- vi@varImportance$corTest # get the vector of importances

ac <- aoa(bio, m, vi=vimp) # the output is the raster layer (current time)



af <- aoa(biof, m, vi=vimp) # for the future

# let's combine them into a single object for visualisation:
a <- c(ac,af)
names(a) <- c('AOA_current','AOA_future')

levelplot(a,margin=F,par.settings = RdBuTheme)

# the pixels less than 0.8 (or less than 1 to be conservative) are not reliable:

# so, let's make a mask:

acm <- ifel(ac >= 0.8,1, NA) # a mask to only keep reliable pixels (current time)
afm <- ifel(af >= 0.8,1, NA) # a mask to only keep reliable pixels (future time)

plot(acm,main='Mask of AoA for the current time')

# let's adjust the predictions/projections (hc, and hf):

hc <- mask(hc, acm) # exclude unreliable pixels for the current time
hf <- mask(hf, afm) # exclude unreliable pixels for the current time

levelplot(c(hc,hf),col.regions=cl(1000),margin=F)

##-----------------------------------------------

## Assess Magnitude of changes:

# we can simply subtract current suitability from the future suitability:

ch <- hf - hc

cl2 <- colorRampPalette(c('black','darkred','red','orange',
                          'yellow','gray','blue','darkblue'))

levelplot(ch,col.regions=cl2(1000),margin=F,main='Magnitude of Gains/Losses')


## Species range mapping and assess range shifts:

# to map ranges, we can convert the habitat suitabilities to presence/absences (P/A)
# the `pa` function can be used for this purpose:
# pa uses a threshold to convert suitability values to P/As

# to extract the threshold, we use the function of `threshold`:
#--- Arguments of getThreshold:
#------> x: sdmModels (m: output of the sdm function)
#------> id: From which model the threshold is extracted (it can be 'ensemble')
#------> opt: which threshold optimisation method should be considered
#------> when id = "ensemble", we provide setting used for the ensemble function


th <- getThreshold(m,id='ensemble',opt=2,setting=list(method='weighted',stat='auc'))

th # the threshold to convert probability values to P/As using the `pa` function:

#--- Arguments of pa:
#------> x: raster of habitat suitability
#------> y: threshold value

pac <- pa(hc, th) # P/As for the current time (baseline)

paf <- pa(hf, th) # P/As for the future time

#-------------
# let's visualise them:

# defining a colorKey for the legend in map:
colKey <- list(at=c(0,0.5,1), ## where the colors change in legend
                   labels=list(
                     at=c(0,1) ## where to print labels
                   ))

levelplot(c(pac,paf),col.regions=c('gray','green'),margin=F,colorkey=colKey)


#-----------------------

## Range shift metrics:

# by subtracting the current P/As from the future one, we can get the following
# values in the map:

#--------------------------
# Future - Current = Change
#--------------------------
#   1    -   0     = 1; Expansion (P in future; A in current)
#   1    -   1     = 0; No Change; persistence (P in future; P in current)
#   0    -   0     = 0; No Change; Unsuitable (A in; A in current)
#   0    -   1     = -1; Contraction or Extinction (A in future; P in current)

#---------------------------

# As you can see above, No change in suitable and unsuitable sites can not be 
# separated, a solution can be to change the 1/0 values in the future layer to 
# a different value, e.g., 100/0 (so, presence sites in the future layer has a 
# value of 100)! To do so, we can simply multiply paf to 100:

paf2 <- paf * 100 

# Now:
#--------------------------
# Future - Current = Change
#--------------------------
#   100   -   0    = 100; Expansion (P in future; A in current)
#   100   -   1    = 99; No Change; Refugia (P in future; P in current)
#   0     -   0    = 0; No Change; Unsuitable (A in; A in current)
#   0     -   1    = -1; Contraction or Extinction (A in future; P in current)

#---------------------------
# so, we have 4 possible values representing range shifts

pach <- paf2 - pac


cls <- data.frame(id=c(-1,0,99,100), 
                  range=c('Extinction', 'Unsuitable_NoChange', 
                          'Refugia','Expansion'))

levels(pach) <- cls


levelplot(pach,att='range',col.regions=c('red','gray','green','blue'),margin=F)

# Refugia is important for biodiversity conservation as it indicates potential 
# locations for species to persist amid climate change, These locations enhance 
# the likelihood of survival for species during climate shifts.

##--------------------------------------

## Metrics of Range Shift assessment:
##----------------------------------

# Here, we quantitatively assess range shifts:

## Area of Contraction (Extinction) and Expansion (Colonization):
# to calculate the area of the pixels representing either of the contraction
# or expansion classes, we first need to identify the pixels of these classes

# By multiplying the number of pixels of each class to the area/size of a pixel,
# we may roughly estimate the area, however, since our dataset uses a lat/lon
# coordinate reference system, the area of pixels are varying across latitudes,
# therefore, we may first calculate the area of each pixel, and then use the areas
# to calculate the size of contraction or expansion:

# we can use the cellSize function to measure the size of pixels:
area <- cellSize(pach, unit='km') # a raster with areas assigned to pixels

# which cells belong to the class of contraction?
# in pach, the value of -1 represents contraction (extinction):

w1 <- which(pach[] == -1) # which cells in pach has the value of -1 (extinction)?

head(w1) # w1 is a vector with cell numbers of pixels for the class of extinction

sum(area[w1]) # Area of Extinction (km2) in the year 2100 based on SSP5 scenario!


w2 <- which(pach[] == 100) # which cells in pach has the value of 100 (expansion)?

sum(area[w2]) # Area of Expansion (km2) in the year 2100 based on SSP5 scenario!

#############################
## Calculate centroid shift:

# to calculate centroid shift, we need to first calculate Xf,Xc, Yf, and Yc which 
# are the mean X and Y (longitudes and latitudes) across pixels of species range 
# in the Current (c) and Future times (f)

# Let's first identify cells of species range for the current and future time

# which pixels in pac (presence/ansence of current time) is equal to 1?
Wc <- which(pac[] == 1) # pixel numbers of species range in the current time
Wf <- which(paf[] == 1) # pixel number of species range in the future time

XYc <- xyFromCell(pac, Wc)# X and Y coordinates of the current time across Wc cells
XYf <- xyFromCell(paf, Wf)# X and Y coordinates of the future time across Wf cells


head(XYc) # let's see how they look like

Xc <- mean(XYc[,1],na.rm=T) # mean of the X column
Yc <- mean(XYc[,2],na.rm=T) # mean of the Y column

Xf <- mean(XYf[,1],na.rm=T) 
Yf <- mean(XYf[,2],na.rm=T) 

Xc
Yc
Xf
Yf

# We can calculate Euclidean distance to measure the shift distance between the 
# future and  current times. However, since the coordinate reference system (CRS) 
# of our layers is geographic, the coordinates of the centroids are also 
# in the unit of decimal degrees (based on latlon CRS). To calculate distance in
# meter, we can use the distGeo function from the package of geosphere 
# to calculate metric distance between two points when they are in geographic 
# coordinates.

# To do so, first let's load the geosphere package (install it if you don't have it)
library(geosphere)

# distance between two points (centroids) based on their coordinates:
dis <- distGeo(c(Xc,Yc),c(Xf,Yf)) / 1000 # divided by 1000 to get distance in KM


dis

## Direction of change:

# we can use the bearing (from the geosphere package) function to get the 
# azimut (direction) from the centroind of current time to the future:


azimuth_deg <- bearing(c(Xc,Yc),c(Xf,Yf))

# we can convert it to a range between 0 to 360 to get
# a clockwise direction--> 0: north; 90: west, 180: south; 270: west; 360: north

if (azimuth_deg < 0) azimuth_deg <- azimuth_deg + 360


print(paste("Centroid shift distance:", round(dis, 2), "km"))
print(paste("Direction of shift:", round(azimuth_deg, 2), "degrees"))



## Latitudinal Shift and 
##----------------------
# we can simply measure the changes between the mean latitude of future (Yf)
# and the mean latitude of the current time (Yc) to get an idea about the 
# latitudinal shift:

latitudinal_shift <- Yf - Yc

if (latitudinal_shift > 0) {
    direction <- "northward"
} else if (latitudinal_shift < 0) {
    direction <- "southward"
} else {
    direction <- "no change"
}

cat("The latitudinal shift is", abs(latitudinal_shift), "degrees,", direction, ".\n")


## Range Area Change:
##--------------------

# Above we extracted the pixel number of species range in both times (Wc and Wf)
# so, we can calculate the area of species range in both times which can be used
# to calculate another metric of range shift assessment: "range area change"

areaC <- sum(area[Wc]) # area of the species range in current time
areaC

areaF <- sum(area[Wf]) # area of the species range in Future
areaF

rangeCh <- ((areaF - areaC) / areaC) * 100
rangeCh # percentage of range change 

# as you can see, almost 26% contraction will be happening to species range in 
# response to climate change (given the worst case scenario: SSP585)

## Area of Overlap:
##-----------------
# Overlap refers to regions that are suitable in both times (refugia), 
# We already mapped these areas in pach (value of 99), so let's get their pixel
# number:

wr <- which(pach[] == 99)

areaR = sum(area[wr]) # Area (km2) of Refugia
areaR

ov <- (areaR / areaC) * 100

ov # Area of overlap (precentage)
```


## Conclusion

In this chapter, we have explored the comprehensive workflow for Species Distribution Modeling (SDM), focusing on the practical application of these models to assess the impacts of climate change on the spatial distribution of species. Starting from downloading and preparing environmental and species occurrence data, to developing robust predictive models, and ultimately applying these models to analyse range shifts, we have highlighted the key methods, tools, and considerations needed to understand and predict how species respond to environmental changes.

One of the core applications of SDM discussed here is the ability to predict current and future distributions of species under different future climate change scenarios. By comparing these distributions, we can assess **potential range shifts**—a crucial step in understanding species vulnerability, adaptation capacity, and potential future habitat availability. These range shifts are often characterised by latitudinal and altitudinal movements, range contractions, and expansions. Through the case study, we demonstrated how to utilise SDM techniques to analyze these shifts, providing valuable insights into the direction and extent of movement as well as the loss or gain of suitable habitats.

Moreover, the chapter underscored the importance of identifying the **Area of Applicability (AOA)** when projecting species distributions globally. This step is critical to ensure that predictions are accurate and minimize the risks of making extrapolations into regions with unfamiliar environmental conditions. 

Throughout this chapter, we used the `sdm` R package [@naimi_sdm_2016], a versatile tool that supports the modeling of SDMs and offers a wide range of functions for species distribution analysis. While we focused on specific aspects of the package, it has much more to explore, including advanced modeling options, evaluation metrics, and ensemble forecasting techniques. We encourage readers to refer to the standards and best practices for developing SDMs as outlined in Araújo et al. (2019), which provides comprehensive guidelines to ensure robust and reliable models [@araujo_standards_2019]. To do so, other relevant packages are available in R to be used along with the `sdm` package to enhance the quality of the models. For instance, the `usdm` package [@naimi2015usdm] offers functions to understand the effects of positional uncertainty on SDMs [@naimi_spatial_2011; @naimi_where_2014], `elsa` provides an entropy-based novel approach [@naimi_elsa_2019], along with other widely used methods, to measure spatial autocorrelation in data, and `rasterdiv` offers tools to characterise ecosystem heterogeneity [@rocchini2021rasterdiv].  Additionally, we recommend the `climetrics` R package [@taheri2024climetrics], which introduces functions to assess multiple dimensions of climate change. This package offers more possibilities to evaluate the impact of climate change on biodiversity [@taheri2016did], enabling researchers to capture a broader perspective on how changing climatic patterns may affect species distributions [@lemes2022dispersal; @gonzalez2024reshuffling; @ebrahimi2023flood].

Climate change represents one of the most significant drivers of biodiversity loss, and the application of SDMs offers a powerful approach to predict and mitigate its effects. Understanding potential range shifts enables conservationists, ecologists, and policymakers to make informed decisions about species management, habitat protection, and the development of wildlife corridors that enhance connectivity between fragmented habitats. By preparing for these shifts, we can better manage species’ resilience and adaptability, reducing the likelihood of local extinctions.

Looking forward, there are several emerging techniques and future directions that can further enhance the utility of SDMs in climate change studies or for many other applications. These include the integration of biotic interactions (e.g., competition, predation, mutualism), land-use change variables, and incorporating genomic data to assess adaptive potential. Additionally, advances in machine learning and artificial intelligence offer new ways to develop more sophisticated models that can handle complex and dynamic ecological processes.

In conclusion, species distribution modeling is a crucial tool in the fight against climate change and biodiversity loss. By accurately predicting the effects of environmental changes on species distributions, we can proactively implement strategies that conserve not only individual species but entire ecosystems. The knowledge and techniques presented in this chapter provide a solid foundation for conducting effective SDM analyses, and underscore the need for continued research, innovation, and collaboration across disciplines to address the multifaceted challenges that lie ahead.


## References
